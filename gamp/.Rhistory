list(
obj_fn = obj_fn,
grad_fn = grad_fn
)
)
}
# now, I can test the code above
#data <- generate_pois_reg_data(n = 1000, p = 1)
#mod <- glm.fit(x = data$X, y = data$y, intercept = FALSE, family = poisson())
# the data are being generated correctly
# now, I want to test if this will work with optim
problem_data <- generate_obj_grad_fn_pos_reg(data$X, data$y)
optim_sol <- optim(
par = rnorm(1),
fn = problem_data$obj_fn,
gr = problem_data$grad_fn,
method = "BFGS"
)
generate_pois_reg_data_offset <- function(n, p) {
X <- matrix(
data = rnorm(n * p, 1, sqrt(.75)), nrow = n, ncol = p
)
b <- runif(p, 0, 1)
eta <- X %*% b
exp_eta <- exp(eta)
c_max <- min(exp_eta)
c <- runif(1, min = 0, max = c_max)
y <- rpois(n = n, lambda = exp_eta - c)
return(
list(
X = X, y = y, b = b, c = c
)
)
}
generate_obj_grad_fn_pos_reg_offset <- function(X, y, c) {
n <- nrow(X)
p <- ncol(X)
obj_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
#obj <- mean(y * log(exp_eta - c) + exp_eta)
obj <- 0
for (i in 1:n) {
obj <- obj - y[i] * log(exp_eta[i] - c) + exp_eta[i]
}
return(obj / n)
}
grad_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
grad <- numeric(p)
for (j in 1:p) {
for (i in 1:n) {
grad[j] <- grad[j] + X[i,j] * exp_eta[i] * ((-y[i] / (exp_eta[i] - c)) + 1)
}
}
return(grad / n)
}
# Now, I need to come up with all of the problem constraints
if (c < 0) {
algorithm_type <- "unconstrained"
ui <- NULL
ci <- NULL
} else {
algorithm_type <- "constrained"
ui <- X
ci <- rep(log(.Machine$double.eps + c), n)
}
return(
list(
obj_fn = obj_fn,
grad_fn = grad_fn,
algorithm_type = algorithm_type,
ui = ui,
ci = ci
)
)
}
generate_obj_grad_fn_pos_reg_offset_fast <- function(X, y, c) {
n <- nrow(X)
obj_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
obj <- mean(-y * log(exp_eta - c) + exp_eta)
return(obj)
}
grad_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
grad <- drop(t((-y / (exp_eta - c)) * exp_eta) %*% X + t(exp_eta) %*% X)
return(grad / n)
}
# Now, I need to come up with all of the problem constraints
if (c <= 0) {
algorithm_type <- "unconstrained"
ui <- NULL
ci <- NULL
} else {
is_count_0 <- (y == 0)
n_constraints <- sum(is_count_0)
algorithm_type <- "constrained"
ui <- X[is_count_0, ]
ci <- rep(log(.Machine$double.eps + c), n_constraints)
}
return(
list(
obj_fn = obj_fn,
grad_fn = grad_fn,
algorithm_type = algorithm_type,
ui = ui,
ci = ci
)
)
}
optim_adam <- function(
obj_fn,
grad_fn,
theta_0,
tol = sqrt(.Machine$double.eps),
alpha = .001,
beta_1 = .9,
beta_2 = .999,
epsilon = 10e-8
) {
iter_diff <- Inf
p <- length(theta_0)
theta <- theta_0
t <- 0
convergence <- FALSE
objective <- do.call(obj_fn, list(theta))
obj_vec <- c(objective)
m <- rep(0, p)
v <- rep(0, p)
steps_since_last_improvement <- 0
while (!convergence) {
t <- t + 1
g <- do.call(grad_fn, list(theta))
m <- beta_1 * m + (1 - beta_1) * g
v <- beta_2 * v + (1 - beta_2) * (g ^ 2)
m_hat <- m / (1 - beta_1 ^ t)
v_hat <- v / (1 - beta_2 ^ t)
theta <- theta - (alpha * m_hat) / (sqrt(v_hat) + epsilon)
objective_t <- do.call(obj_fn, list(theta))
improvement <- -1 * (objective - objective_t)
rel_improvement <- abs(improvement / objective)
if (rel_improvement < tol) {
steps_since_last_improvement <- steps_since_last_improvement + 1
if (steps_since_last_improvement >= 5) {
convergence <- TRUE
}
} else {
steps_since_last_improvement <- 0
}
objective <- objective_t
obj_vec <- c(obj_vec, objective)
if (t %% 10 == 0) {
print(glue::glue("Step {t}, improvement of {rel_improvement * 100}% in objective"))
}
}
return(
list(
theta = theta,
obj_vec = obj_vec
)
)
}
optim_gd <- function(
obj_fn,
grad_fn,
init,
tol = sqrt(.Machine$double.eps),
learning_rate = .000001
) {
iter_diff <- Inf
params <- init
obj_val <- do.call(obj_fn, list(params))
n_iter <- 0
obj_vec <- c(obj_val)
while (iter_diff > tol) {
grad <- do.call(grad_fn, list(params))
params <- params - learning_rate * grad
new_obj_val <- do.call(obj_fn, list(params))
iter_diff <- -1 * (new_obj_val - obj_val)
obj_val <- new_obj_val
obj_vec <- c(obj_vec, obj_val)
n_iter <- n_iter + 1
if (n_iter %% 10 == 0) {
print(glue::glue("Iteration {n_iter}, obj decrease {iter_diff}"))
}
}
return(
list(
sol = params,
obj_vec = obj_vec,
n_iter = n_iter
)
)
}
data <- generate_pois_reg_data_offset(n = 5000, p = 20)
problem_data <- generate_obj_grad_fn_pos_reg_offset(data$X, data$y, data$c)
problem_data_fast <- generate_obj_grad_fn_pos_reg_offset_fast(data$X, data$y, data$c)
init <- rnorm(20)
data$c
data$y
sum(data$y == 0)
generate_pois_reg_data <- function(n, p) {
X <- matrix(
data = rnorm(n * p, 0, 3), nrow = n, ncol = p
)
b <- rnorm(p, 0, 3)
eta <- X %*% b
y <- rpois(n = n, lambda = exp(eta))
return(
list(
X = X, y = y, b = b
)
)
}
generate_obj_grad_fn_pos_reg <- function(X, y) {
n <- nrow(X)
p <- ncol(X)
obj_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
obj <- 0
for (i in 1:n) {
obj <- obj + exp_eta[i] - y[i] * eta[i]
}
return(obj / n)
}
grad_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
grad <- numeric(p)
for (j in 1:p) {
for (i in 1:n) {
grad[j] <- grad[j] + X[i, j] * (exp_eta[i] - y[i])
}
}
return(grad / n)
}
return(
list(
obj_fn = obj_fn,
grad_fn = grad_fn
)
)
}
# now, I can test the code above
#data <- generate_pois_reg_data(n = 1000, p = 1)
#mod <- glm.fit(x = data$X, y = data$y, intercept = FALSE, family = poisson())
# the data are being generated correctly
# now, I want to test if this will work with optim
problem_data <- generate_obj_grad_fn_pos_reg(data$X, data$y)
optim_sol <- optim(
par = rnorm(1),
fn = problem_data$obj_fn,
gr = problem_data$grad_fn,
method = "BFGS"
)
generate_pois_reg_data_offset <- function(n, p) {
X <- matrix(
data = rnorm(n * p, .5, sqrt(.5)), nrow = n, ncol = p
)
b <- runif(p, 0, .75)
eta <- X %*% b
exp_eta <- exp(eta)
c_max <- min(exp_eta)
c <- runif(1, min = 0, max = c_max)
y <- rpois(n = n, lambda = exp_eta - c)
return(
list(
X = X, y = y, b = b, c = c
)
)
}
generate_obj_grad_fn_pos_reg_offset <- function(X, y, c) {
n <- nrow(X)
p <- ncol(X)
obj_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
#obj <- mean(y * log(exp_eta - c) + exp_eta)
obj <- 0
for (i in 1:n) {
obj <- obj - y[i] * log(exp_eta[i] - c) + exp_eta[i]
}
return(obj / n)
}
grad_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
grad <- numeric(p)
for (j in 1:p) {
for (i in 1:n) {
grad[j] <- grad[j] + X[i,j] * exp_eta[i] * ((-y[i] / (exp_eta[i] - c)) + 1)
}
}
return(grad / n)
}
# Now, I need to come up with all of the problem constraints
if (c < 0) {
algorithm_type <- "unconstrained"
ui <- NULL
ci <- NULL
} else {
algorithm_type <- "constrained"
ui <- X
ci <- rep(log(.Machine$double.eps + c), n)
}
return(
list(
obj_fn = obj_fn,
grad_fn = grad_fn,
algorithm_type = algorithm_type,
ui = ui,
ci = ci
)
)
}
generate_obj_grad_fn_pos_reg_offset_fast <- function(X, y, c) {
n <- nrow(X)
obj_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
obj <- mean(-y * log(exp_eta - c) + exp_eta)
return(obj)
}
grad_fn <- function(b) {
eta <- X %*% b
exp_eta <- exp(eta)
grad <- drop(t((-y / (exp_eta - c)) * exp_eta) %*% X + t(exp_eta) %*% X)
return(grad / n)
}
# Now, I need to come up with all of the problem constraints
if (c <= 0) {
algorithm_type <- "unconstrained"
ui <- NULL
ci <- NULL
} else {
is_count_0 <- (y == 0)
n_constraints <- sum(is_count_0)
algorithm_type <- "constrained"
ui <- X[is_count_0, ]
ci <- rep(log(.Machine$double.eps + c), n_constraints)
}
return(
list(
obj_fn = obj_fn,
grad_fn = grad_fn,
algorithm_type = algorithm_type,
ui = ui,
ci = ci
)
)
}
optim_adam <- function(
obj_fn,
grad_fn,
theta_0,
tol = sqrt(.Machine$double.eps),
alpha = .001,
beta_1 = .9,
beta_2 = .999,
epsilon = 10e-8
) {
iter_diff <- Inf
p <- length(theta_0)
theta <- theta_0
t <- 0
convergence <- FALSE
objective <- do.call(obj_fn, list(theta))
obj_vec <- c(objective)
m <- rep(0, p)
v <- rep(0, p)
steps_since_last_improvement <- 0
while (!convergence) {
t <- t + 1
g <- do.call(grad_fn, list(theta))
m <- beta_1 * m + (1 - beta_1) * g
v <- beta_2 * v + (1 - beta_2) * (g ^ 2)
m_hat <- m / (1 - beta_1 ^ t)
v_hat <- v / (1 - beta_2 ^ t)
theta <- theta - (alpha * m_hat) / (sqrt(v_hat) + epsilon)
objective_t <- do.call(obj_fn, list(theta))
improvement <- -1 * (objective - objective_t)
rel_improvement <- abs(improvement / objective)
if (rel_improvement < tol) {
steps_since_last_improvement <- steps_since_last_improvement + 1
if (steps_since_last_improvement >= 5) {
convergence <- TRUE
}
} else {
steps_since_last_improvement <- 0
}
objective <- objective_t
obj_vec <- c(obj_vec, objective)
if (t %% 10 == 0) {
print(glue::glue("Step {t}, improvement of {rel_improvement * 100}% in objective"))
}
}
return(
list(
theta = theta,
obj_vec = obj_vec
)
)
}
optim_gd <- function(
obj_fn,
grad_fn,
init,
tol = sqrt(.Machine$double.eps),
learning_rate = .000001
) {
iter_diff <- Inf
params <- init
obj_val <- do.call(obj_fn, list(params))
n_iter <- 0
obj_vec <- c(obj_val)
while (iter_diff > tol) {
grad <- do.call(grad_fn, list(params))
params <- params - learning_rate * grad
new_obj_val <- do.call(obj_fn, list(params))
iter_diff <- -1 * (new_obj_val - obj_val)
obj_val <- new_obj_val
obj_vec <- c(obj_vec, obj_val)
n_iter <- n_iter + 1
if (n_iter %% 10 == 0) {
print(glue::glue("Iteration {n_iter}, obj decrease {iter_diff}"))
}
}
return(
list(
sol = params,
obj_vec = obj_vec,
n_iter = n_iter
)
)
}
data <- generate_pois_reg_data_offset(n = 5000, p = 20)
problem_data <- generate_obj_grad_fn_pos_reg_offset(data$X, data$y, data$c)
problem_data_fast <- generate_obj_grad_fn_pos_reg_offset_fast(data$X, data$y, data$c)
init <- rnorm(20)
sum(data$y == 0)
data$y
max(data$y)
problem_data_fast$ui
problem_data_fast$ci
data$c
problem_data_fast$obj_fn(init)
problem_data_fast$obj_fn(rep(0, 20))
help("constrOptim")
constrOptim_test_fast <- constrOptim(
rep(0, 20),
problem_data_fast$obj_fn,
problem_data_fast$grad_fn,
problem_data_fast$ui,
problem_data_fast$ci
)
constrOptim_test_fast$par
data$b
help(optimize)
setwd("~/Documents/academic/gwas_amplification/paper_figures")
# Here, I just want to test simulation of the new pgs corr methodology
set.seed(8675309)
'%>%' <- magrittr::'%>%'
library(ggplot2)
Sigma <- list()
#Sigma[[1]] <- (.25 ^ 2) * matrix(data = 1, nrow = 2, ncol = 2)
#Sigma[[2]] <- (.25 ^ 2) * matrix(data = c(1, 0, 0, 0), nrow = 2, ncol = 2)
#Sigma[[3]] <- (.25 ^ 2) * matrix(data = c(0, 0, 0, 1), ncol = 2)
Sigma[[1]] <- (.25 ^ 2) * matrix(data = 1, nrow = 2, ncol = 2)
Sigma[[2]] <- (.25 ^ 2) * matrix(data = c(1.5, sqrt(1.5), sqrt(1.5), 1), ncol = 2)
Sigma[[3]] <- (.25 ^ 2) * matrix(data = c(1, sqrt(1.5), sqrt(1.5), 1.5), ncol = 2)
maf_simulator <- function(n) {
pmax(pmin(rbeta(n, .5, 10), .5 - 1e-05), .005)
}
metric_vec <- c()
model_type_vec <- c()
selection_method_vec <- c()
value_vec <- c()
amp_vec <- c()
for (amp in seq(.5, 1, .1)) {
print(amp)
sim_out <- gamp::simulate_pgs_corr_fast_v2(
n_e0 = 13333,
n_e1 = 13333,
n_snps = 5000,
maf_simulator,
e0_h2 = .25,
e1_h2 = .25,
Sigma = Sigma,
pi = c(1 - amp, amp/2, amp/2),
num_sims = 10,
s = .5,
pval_thresh_additive = .1,
pval_thresh_GxE = .1
)
for (model_type in c("additive", "GxE")) {
for (metric in c("mse", "power", "type_1_error", "accuracy", "corr")) {
for (selection_method in c("pval", "same_number")) {
metric_vec <- c(metric_vec, metric)
model_type_vec <- c(model_type_vec, model_type)
value_vec <- c(value_vec, sim_out[[selection_method]][[model_type]][[metric]])
selection_method_vec <- c(selection_method_vec, selection_method)
amp_vec <- c(amp_vec, amp)
}
}
}
readr::write_rds(sim_out, glue::glue("rds_data/polymetric_{amp}_zhu_test.rds"))
}
